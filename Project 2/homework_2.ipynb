{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1- small data learning with embeddings (30%)\n",
    "### We're going to use pre-trained embeddings to try to learn a text classification problem with few training examples\n",
    "### This is very similar to what we did in class!\n",
    "## $ \\\\ $\n",
    "## Part 0: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=1234)\n",
    "data_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'), random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'About a year ago I started work on a problem that appeared to\\nbe very simple and turned out to be quite difficult.  I am wondering if\\nanyone on the net has seen this problem and (hopefully) some published \\nsolutions to it.\\n\\n\\tThe problem is to draw an outline of a surface defined by two\\nroughly parallel cubic splines.  For inputs the problem essentially\\nstarts with two sets of points where each set of points is on the \\nedge of an object which we treat as two dimensional, i.e. only extant\\nbetween the edges, but which exists in three dimensional space.  To draw \\nthe object we \\n\\n1) fit a cubic spline through the points.  Each spline is effectively\\n\\tcomputed as a sequence of line segments approximating the\\n        curve.  Each spline has an equal number of segments.  We assume\\n\\tthat the nth segment along each spline is roughly, but not\\n\\texactly, the same distance along each spline by any reasonable\\n\\tmeasure.\\n2) Take each segment (n) along each spline and match it to the nth segment\\n\\tof the opposing spline.  Use the pair of segments to form two\\n\\ttriangles which will be filled in to color the surface.\\n3) Depth sort the triangles\\n4) Take each triangle in sorted order, project onto a 2D pixmap, draw\\n\\tand color the triangle.  Take the edge of the triangle that is\\n\\talong the edge of the surface and draw a line along that edge\\n\\tcolored with a special \"edge color\"\\n\\n\\tIt is the edge coloring in step 4 that is at the heart of the\\nproblem.  The idea is to effectively outline the edge of the surface.\\nThe net result however generally has lots of breaks and gaps in\\nthe edge of the surface.  The reasons for this are fairly complicated.\\nThey involve both rasterization problems and problems resulting\\nfrom the projecting the splines.  If anything about this problem\\nsounds familiar we would appreciate knowing about other work in this\\narea.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:\n",
    "### a. What is the most common class in the train set?\n",
    "### b. What is the out of sample (test) accuracy if we guess the most probable class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common class 10: rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "# Way 1 to find most common class using categorical\n",
    "def preprocess_targets(target, num_classes):\n",
    "    return to_categorical(target, num_classes)\n",
    "\n",
    "num_classes = np.unique(data_train.target).shape[0]\n",
    "y_train = preprocess_targets(data_train.target, num_classes)\n",
    "y_test = preprocess_targets(data_test.target, num_classes)\n",
    "most_common_class = np.argmax(y_train.sum(axis=0))\n",
    "print('most common class {}: {}'.format(most_common_class, data_train.target_names[most_common_class]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common class 10: rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "# Way 2 to find most common class using numpy\n",
    "most_common_class = np.argmax(np.bincount(data_train.target))\n",
    "print('most common class {}: {}'.format(most_common_class, data_train.target_names[most_common_class]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.0530\n"
     ]
    }
   ],
   "source": [
    "# With way 1 previously\n",
    "print('f1 {:.4f}'.format(accuracy_score(np.argmax(y_test, axis=1), most_common_class * np.ones(y_test.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.0530\n"
     ]
    }
   ],
   "source": [
    "# With way 2 previously\n",
    "print('f1 {:.4f}'.format(accuracy_score(data_test.target, most_common_class * np.ones(y_test.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6abb69d4c2d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mterm_document_matrix_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_document_matrix_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# Do they mean this or the other one????\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(lowercase=False, )\n",
    "vec.fit(data_train.data)\n",
    "term_document_matrix_train = vec.transform(data_train.data)\n",
    "term_document_matrix_test = vec.transform(data_test.data)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(term_document_matrix_train, data_train.target)\n",
    "\n",
    "lr_preds = lr.predict(term_document_matrix_test)\n",
    "print('accuracy {:.4f}'.format(accuracy_score(data_test.target, lr_preds)))\n",
    "print('f1 {:.4f}'.format(f1_score(data_test.target, lr_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professor's code\n",
    "#print('f1 {:.4f}'.format(f1_score(data_test.target, lr_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Turn the text into integer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 10000\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "## this refers to int_sequences_train\n",
    "# 1. Instantiate a tokenizer with max words\n",
    "# 2. fit the tokenizer on text\n",
    "# 3. Turn the text into integer sequences (train and test)\n",
    "# 4. pad the sequences to a constant sequence length (train and test)\n",
    "## this refers to y_train\n",
    "# 5. turn y into categorical variables\n",
    "\n",
    "\n",
    "# you should have 4 variables:\n",
    "# y_train, y_test, int_sequences_train, int_sequences_test\n",
    "# all are numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting y_train and y_test\n",
    "def preprocess_targets(target, num_classes):\n",
    "    return to_categorical(target, num_classes)\n",
    "\n",
    "num_classes = np.unique(data_train.target).shape[0]\n",
    "y_train = preprocess_targets(data_train.target, num_classes)\n",
    "y_test = preprocess_targets(data_test.target, num_classes)\n",
    "\n",
    "\n",
    "# Getting int_sequences_train and int_sequences_test\n",
    "tok = Tokenizer(num_words=MAX_WORDS)\n",
    "tok.fit_on_texts(data_train.data)\n",
    "\n",
    "int_sequences_train = tok.texts_to_sequences(data_train.data)\n",
    "int_sequences_test = tok.texts_to_sequences(data_test.data)\n",
    "int_sequences_train = pad_sequences(int_sequences_train, maxlen=MAX_SEQ_LEN)\n",
    "int_sequences_test = pad_sequences(int_sequences_test, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_train.shape ==(11314, 20), 'something went wrong'\n",
    "assert int_sequences_test.shape == (7532, 100), 'something went wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: load the GloVe embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GLOVE_DIR = '/Users/hermannviktor/Python/Databases'  # FIXME directory with glove\n",
    "GLOVE_PATH = os.path.join(GLOVE_DIR, 'glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_file(filepath):\n",
    "    word_to_vector = {}\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_to_vector[word] = vector\n",
    "    return word_to_vector\n",
    "\n",
    "word_vecs = load_glove_file(GLOVE_PATH)\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "for word, i in tok.word_index.items():\n",
    "    if i >= MAX_WORDS:\n",
    "        continue\n",
    "    embedding_vector = word_vecs.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = y_train.shape[1]\n",
    "assert NUM_CLASSES == 20, 'something went wrong'\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dropout, Dense, GlobalAveragePooling1D\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "# TODO\n",
    "# 1. Build a model with\n",
    "#  - an embedding\n",
    "#  - some number of dense layers\n",
    "#  - dropout\n",
    "#  - don't forget to use GlobalAveragePooling to average over one dimension\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "word_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "\n",
    "hidden_state = Embedding(\n",
    "    MAX_WORDS, \n",
    "    EMBEDDING_DIM,\n",
    "    embeddings_initializer=Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    input_length=MAX_SEQ_LEN)(word_input)\n",
    "hidden_state = Dropout(0.2)(hidden_state)\n",
    "hidden_state = Dense(128, activation='relu')(hidden_state)\n",
    "hidden_state = Dropout(0.5)(hidden_state)\n",
    "hidden_state = GlobalAveragePooling1D()(hidden_state)\n",
    "hidden_state = Dense(64, activation='relu')(hidden_state)\n",
    "hidden_state = Dropout(0.5)(hidden_state)\n",
    "\n",
    "output = Dense(NUM_CLASSES, activation='sigmoid')(hidden_state)\n",
    "\n",
    "model = Model(word_input, output)\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516084"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_to_train = 100\n",
    "epochs = 1000  # this is a big number but won't take long with 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19622942113648434"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    int_sequences_train[:num_samples_to_train], \n",
    "    y_train[:num_samples_to_train], \n",
    "    epochs=1000, shuffle=True, batch_size=num_samples_to_train, verbose=0\n",
    ")\n",
    "accuracy_score(np.argmax(y_test, axis=1), np.argmax(model.predict(int_sequences_test), axis=1).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## you should be able to get more than 20% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Compare to others methods\n",
    "### a. How does this compare to a randomly initialized, trainable embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Build the same model as above, but with a random embedding\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "word_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "\n",
    "hidden_state = Embedding(\n",
    "    MAX_WORDS, \n",
    "    EMBEDDING_DIM,\n",
    "    embeddings_initializer=Constant(embedding_matrix),\n",
    "    trainable=True,\n",
    "    input_length=MAX_SEQ_LEN)(word_input)\n",
    "hidden_state = Dropout(0.2)(hidden_state)\n",
    "hidden_state = Dense(128, activation='relu')(hidden_state)\n",
    "hidden_state = Dropout(0.5)(hidden_state)\n",
    "hidden_state = GlobalAveragePooling1D()(hidden_state)\n",
    "hidden_state = Dense(64, activation='relu')(hidden_state)\n",
    "hidden_state = Dropout(0.5)(hidden_state)\n",
    "\n",
    "output = Dense(NUM_CLASSES, activation='sigmoid')(hidden_state)\n",
    "\n",
    "model = Model(word_input, output)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10116834838024429"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    int_sequences_train[:num_samples_to_train], \n",
    "    y_train[:num_samples_to_train], \n",
    "    epochs=1000, shuffle=True, batch_size=num_samples_to_train, verbose=0\n",
    ")\n",
    "accuracy_score(np.argmax(y_test, axis=1), np.argmax(model.predict(int_sequences_test), axis=1).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b: how does this compare to logistic regression trained on 100 samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hermannviktor/anaconda3/envs/MIT/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/hermannviktor/anaconda3/envs/MIT/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12413701540095592"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "data_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# TODO\n",
    "# 1. make a count vectorizer\n",
    "# 2. fit it on only `samples_to_train` data points\n",
    "# 3. trainsform train and test data into integers\n",
    "# 4. fit logistic regression on just `num_samples_to_train` samples\n",
    "# 5. Compute accuracy score\n",
    "\n",
    "vec = CountVectorizer()\n",
    "\n",
    "# your code here\n",
    "\n",
    "vec.fit(data_train.data)\n",
    "term_document_matrix_train = vec.transform(data_train.data)\n",
    "term_document_matrix_test = vec.transform(data_test.data)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(term_document_matrix_train[:num_samples_to_train], data_train.target[:num_samples_to_train])\n",
    "\n",
    "accuracy_score(data_test.target, lr.predict(term_document_matrix_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This should be approximately 10-12%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Homework problem: improving BOW (30%)\n",
    "There are many improvements that can be made to the bag of words represetation, without resorting to neural networks.\n",
    "Here we'll try one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe to restart notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: fit a bag of words and logistic regression to the 20 newsgroups data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "data_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo\n",
    "# 1. make a count vectorizer with max_features=20000\n",
    "# 2. fit it\n",
    "# 3. transform the train and test data into number\n",
    "vec = CountVectorizer(max_features=20000)\n",
    "vec.fit(data_train.data)\n",
    "xtr = vec.transform(data_train.data)\n",
    "xte = vec.transform(data_test.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6064790228359002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "# 1. fit logistic regression\n",
    "# 2. compute accuracy score\n",
    "lr = LogisticRegression()\n",
    "lr.fit(xtr, data_train.target)   # to be removed\n",
    "accuracy_score(data_test.target, lr.predict(xte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: TFIDF\n",
    "A big problem with counting words is that we'll tend to overweight very common words. \n",
    "These common words often carry little information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 93969),\n",
       " ('to', 51191),\n",
       " ('of', 45608),\n",
       " ('a', 40042),\n",
       " ('and', 39197),\n",
       " ('is', 28204),\n",
       " ('in', 27756),\n",
       " ('I', 27143),\n",
       " ('that', 25016),\n",
       " ('for', 18066)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def word_iterator():\n",
    "    \"\"\"This iterator yields one word at a time from the train data\"\"\"\n",
    "    for doc in data_train.data:\n",
    "        for word in doc.split():\n",
    "            yield word\n",
    "\n",
    "Counter(word_iterator()).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF is a scheme that combats this. \n",
    "## TFIDF = $\\text{Term Frequency Inverse Document Frequency} $\n",
    "# $ \\\\ $\n",
    "\n",
    "# $TFIDF\\left(d, t\\right) \\equiv \\frac{\\text{ Count}\\left(d, t\\right)}{\\text{Doc-Freq}\\left(d, t\\right)} \\equiv \\text{ Count}\\left(d, t\\right)\\,\\left(1 + log\\left( \\frac{N_{docs}}{df_{t}}\\right)\\right)$\n",
    "## Where\n",
    "### $df_{t}$ is the number of documents in which term $t$ appears\n",
    "### $N_{docs}$ is the total number of documents\n",
    "### $\\text{Count}\\left(d,t\\right)$ is the number of times term $t$ appears in document $d$ (the count matrix)\n",
    "\n",
    "# $ \\\\ $ \n",
    "# $ \\\\ $ \n",
    "## Like this, we suppress the weight of common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a: write turn the count matrix into a TFIDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf_vector(count_matrix):\n",
    "    \"\"\"Get the inverse document frequence vector (shape = num_words)\"\"\"\n",
    "    df = np.array((count_matrix > 0).astype(int).sum(axis=0))\n",
    "    return np.log(count_matrix.shape[0] / (1+df))\n",
    "\n",
    "\n",
    "#TODO(fill in this function)\n",
    "def get_tfidf_matrix(count_matrix):\n",
    "    \"\"\"Turn a count matrix into a tfidf matrix\"\"\"\n",
    "    # TODO\n",
    "#     1. get the idfs with the aboce function\n",
    "#     2. turn it into a numpy array `with .toarray()`\n",
    "#     3. loop the the ROWS of the matrix and transform them\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_transformed = get_tfidf_matrix(xtr)\n",
    "xte_transformed = get_tfidf_matrix(xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It worked!\n"
     ]
    }
   ],
   "source": [
    "assert xtr_transformed.shape == xtr.shape, 'something has gone wrong'\n",
    "print('It worked!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6251991502920871"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(xtr_transformed, data_train.target)\n",
    "accuracy_score(data_test.target, lr.predict(xte_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Do the same with scikitlearn's implmenetation\n",
    "### Happily, sklearn does this for us\n",
    "### And it includes some other nice normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6715347849176846"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TODO:\n",
    "# 1. instantiate a TfidfVectorizer with max_features = 20000\n",
    "# 2. fit it on the train data\n",
    "# 3. transform train and test data into matrices\n",
    "# 4. fit logistic regression on the train data\n",
    "# 5. compute the accuracy score on the test data\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer(max_features=20000)\n",
    "\n",
    "# Your code here\n",
    "\n",
    "accuracy_score(data_test.target, lr.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Tuning the number of words to use\n",
    "## Make a plot of how the vocabulary size (`max_features`) impacts results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this took 59.59 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "results = {}\n",
    "for max_features in (100, 500, 1000, 5000, 10000, 20000, 50000, None):\n",
    "    # TODO:\n",
    "    # 1. instantiate a TfidfVectorizer with max_features = max_features\n",
    "    # 2. fit it on the train data\n",
    "    # 3. transform train and test data into matrices\n",
    "    # 4. fit logistic regression on the train data\n",
    "    # 5. compute the accuracy score on the test data\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    # x_train = ...\n",
    "    # x_test = ...\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "    # lr.fit(...\n",
    "    \n",
    "    if max_features is None:\n",
    "        num_features = len(vec.get_feature_names())\n",
    "    else:\n",
    "        num_features = max_features\n",
    "    results[num_features] = accuracy_score(data_test.target, lr.predict(x_test)) # to be removed\n",
    "\n",
    "print('this took {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(results).plot(figsize=(12,8), fontsize=16)\n",
    "plt.xlabel('max features', fontsize=16)\n",
    "plt.ylabel('out of sample accuracy', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Named entity recognition (40 %)\n",
    "\n",
    "Named entity recognition is a common NLP task that tries to identify entities in text.\n",
    "\n",
    "See: https://en.wikipedia.org/wiki/Named-entity_recognition\n",
    "\n",
    "Common Types of entities include `Locations`, `People`, and `Organizations`. For example, in the sentence\n",
    "# Janet Yellen, the chairwoman of the Federal Reserve, gave a speech in Colorado.\n",
    "## $ \\\\ $ \n",
    "the goal would be to recognize\n",
    "# `Janet Yellen`$_{PERSON}$, the chairwoman of the `Federal Reserve`$_{ORGANIZATION}$, gave a speech in `Colorado`$_{LOCATION}$.\n",
    "# $ \\\\ $\n",
    "# In this problem we will build a model to recognized named entities using word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: \n",
    "### Give an example of a sentence with a Person but not a location. \n",
    "### Give an example of a sentence with an organization and a location, but not a person. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: building a model\n",
    "# $ \\\\ $\n",
    "### The goal of this section is to build a model to take a sentence (list of words) and identify what kind of tag each word is\n",
    "# $ \\\\ $\n",
    "## Why is this problem hard:\n",
    "### Some words will be the same tag all the time. For example `Colorado` is almost always a `LOCATION`\n",
    "### Some words depend on context: above `federal` and `reserve` are `ORGANIZATION` but I can write `I would like to reserve a table.`\n",
    "\n",
    "# $ \\\\ $\n",
    "## To combat this issue we will make a very simple model but taking a 3-word window around every word\n",
    "  - For every word, we will take the word vector of that word and the two surrounding words\n",
    "\n",
    "### Example: `I went to the store` will be represented as \n",
    " - `I` $\\rightarrow$ `UKNOWN` - `I` - `went` $\\rightarrow$ $\\left[ V_{UKNOWN}, V_{I}, V_{went}\\right]$\n",
    " - `went` $\\rightarrow$ `I` - `went` - `to`$\\rightarrow$ $\\left[V_{I}, V_{went},  V_{to}\\right]$\n",
    " - `to` $\\rightarrow$ `went` - `to` - `the`$\\rightarrow$ $\\left[V_{went},  V_{to}, V_{the}\\right]$\n",
    " - ...\n",
    "#### Where \n",
    " - $V_{word_{i}}$ is the representation for $word_{i}$\n",
    " - `UKNOWN` is the token for unknown or boundary words\n",
    " \n",
    "Like this, we will encode some __context__ around every word. Each word here will be encoded as a $3 * d_{embedding}$-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_file(filepath):\n",
    "    \"\"\"Load a glove embedding from a file\"\"\"\n",
    "    word_to_vector = {}\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_to_vector[word] = vector\n",
    "    return word_to_vector\n",
    "\n",
    "def load_dataset(fname):\n",
    "    \"\"\"Load an NER dataset\"\"\"\n",
    "    docs = []\n",
    "    with open(fname) as fd:\n",
    "        cur = []\n",
    "        for line in fd:\n",
    "            line = line.lower()\n",
    "            # new sentence on -DOCSTART- or blank line\n",
    "            if re.match(r\"-DOCSTART-.+\".lower(), line) or (len(line.strip()) == 0):\n",
    "                if len(cur) > 0:\n",
    "                    docs.append(cur)\n",
    "                cur = []\n",
    "            else: # read in tokens\n",
    "                cur.append(line.strip().split(\"\\t\",1))\n",
    "        # flush running buffer\n",
    "        if cur:\n",
    "            docs.append(cur)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GLOVE_DIR = ''  # FIXME directory with glove\n",
    "DATA_PATH = ''  # where you downloaded the data\n",
    "\n",
    "word_vecs = load_glove_file(os.path.join(GLOVEIDR, 'glove.6B.50d.txt'))\n",
    "docs = load_dataset(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_first_doc = [\n",
    "    ['eu', 'org'],\n",
    "    ['rejects', 'o'],\n",
    "    ['german', 'misc'],\n",
    "    ['call', 'o'],\n",
    "    ['to', 'o'],\n",
    "    ['boycott', 'o'],\n",
    "    ['british', 'misc'],\n",
    "    ['lamb', 'o'],\n",
    "    ['.', 'o'],\n",
    "]\n",
    "assert len(word_vecs) == 400000, 'word vectors did not load properly'\n",
    "assert word_vecs['the'].shape == (50,), 'word vectors did not load properly'\n",
    "assert len(docs) == 14041, 'something has gone wrong with data loading'\n",
    "assert docs[0] == correct_first_doc, 'something has gone wrong with data loading'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = len(word_vecs)  # max number of words to use in the embedding\n",
    "UNKNOWN = 'UUUNKKK'.lower()  # token for unknown word\n",
    "UNKNOWN_WORD_INDEX = 0\n",
    "EMBEDDING_DIM = 50  # dimension of embedding\n",
    "NULL_TAG = 'o'  # tags that are not a named entity\n",
    "\n",
    "# Some derived quantities\n",
    "TAGS = (NULL_TAG, 'loc', 'per', 'org', 'misc')\n",
    "NUM_TO_TAG = dict(enumerate(TAGS))\n",
    "TAG_TO_NUM = {tag: num for num, tag in NUM_TO_TAG.items()}\n",
    "\n",
    "NUM_CLASSES = len(TAGS)\n",
    "assert NUM_CLASSES == 5, 'somethig has gone wrong'\n",
    "\n",
    "WINDOW = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_num = {word: idx + 1 for idx, word in enumerate(word_vecs.keys())}\n",
    "num_to_word = {num: word for word, num in word_to_num.items()}\n",
    "\n",
    "word_to_num[UNKNOWN] = UNKNOWN_WORD_INDEX\n",
    "num_to_word[UNKNOWN_WORD_INDEX] = UNKNOWN\n",
    "\n",
    "assert word_to_num['the'] < 10, '\"the\" is not a common word- something has gone wrong.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix\n",
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "for word, i in word_to_num.items():#tok.word_index.items():\n",
    "    if i >= MAX_WORDS:\n",
    "        continue\n",
    "    embedding_vector = word_vecs.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating windowed-word sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_windows(words, tags, word_to_num, tag_to_num, left=WINDOW, right=WINDOW):\n",
    "    \"\"\"Turn sequences of words and tags into corresponding windowed sequences\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    word_dict = {ind: word for ind, word in enumerate(words)}\n",
    "    for i, word in enumerate(words):\n",
    "        if word == \"<s>\" or word == \"</s>\":\n",
    "            continue # skip sentence delimiters\n",
    "        word_seq = [word_dict.get(i + ii, UNKNOWN) for ii in range(-left, 1 + right)]\n",
    "        int_seq = [word_to_num.get(w, UNKNOWN_WORD_INDEX) for w in word_seq]\n",
    "        tagn = tag_to_num[tags[i]] \n",
    "        X.append(int_seq)\n",
    "        y.append(tagn)\n",
    "    return array(X), array(y)\n",
    "\n",
    "\n",
    "def window_row_to_vector(window_row, embed_matrix):\n",
    "    \"\"\"Turn a row of integers (np.array) into a single word vector\"\"\"\n",
    "    # TODO: implement this\n",
    "    return np.hstack([embed_matrix[i] for i in window_row]) # to be removed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = zip(*docs[0])\n",
    "x, y = seq_to_windows(words, tags, word_to_num=word_to_num, tag_to_num=TAG_TO_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x.dtype == np.int, 'x has the wrong data type'\n",
    "reconstructed_words = [num_to_word[num] for num in x[:, WINDOW]]\n",
    "assert tuple(reconstructed_words) == words, 'word transformation has gone wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xs = []\n",
    "all_ys = []\n",
    "for doc in docs:\n",
    "#     TODO\n",
    "#     1. unpack the words and the tags from `docs`\n",
    "#     2. use `seq_to_windows` to turn `words` and `tag` into `x` and `y`\n",
    "#     3. turn `x` into a single vector with `window_row_to_vector`\n",
    "    \n",
    "    words, tags = zip(*doc)\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    all_xs.extend(x)\n",
    "    all_ys.extend(y)\n",
    "    \n",
    "    \n",
    "all_xs = np.vstack(all_xs)\n",
    "all_ys = np.vstack(all_ys)\n",
    "\n",
    "\n",
    "all_ys = to_categorical(all_ys)\n",
    "assert all_xs.shape[0] == all_ys.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. make an array of indices to be shuffled with `np.arange`\n",
    "# 2. shuffle the indices randomly\n",
    "# 3. use the shuffled indices to shuffle `all_xs` and `all_ys`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# inds = np.arange(....\n",
    "\n",
    "cut = int(0.8 * all_xs.shape[0])\n",
    "x_train, x_val = all_xs[:cut], all_xs[cut:]\n",
    "y_train, y_val = all_ys[:cut], all_ys[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "#TODO\n",
    "# 1. build a network with \n",
    "#  - an input layer\n",
    "#  - some number of dense layers and dropout\n",
    "\n",
    "word_input = Input(shape=(x_train.shape[1],))  # to be removed\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "#output = ...\n",
    "\n",
    "model = Model(word_input, output)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), shuffle=True, epochs=16, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to process new sentences so that they can be processed by our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"Preprocess a sentence into word vectors for the model\n",
    "    \n",
    "    TODO:\n",
    "        1. split sentence into words (and make lowercase)\n",
    "        2. make each word into a 3-word window (use seq_to_windows)\n",
    "            - this will require the creating some fake tags in the right format\n",
    "                in order to pass to `seq_to_windows`\n",
    "        3. turn each row (3-word window) into a single vector (use window_row_to_vector)\n",
    "        4. turn the list of 150-d vectors into a numpy matrix shape (n_words x 150)\n",
    "    \"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    # your code here\n",
    "    #fake_tags = ... we don't know the tags\n",
    "    # your code here\n",
    "    #return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    preprocess_sentence('This sentence has five words').shape == \n",
    "    (5, EMBEDDING_DIM * (1 + 2 * WINDOW))\n",
    "), '`preprocess_sentence` does not work'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eric      :  per\n",
      "Schmidt   :  per\n",
      "quit      :  o\n",
      "after     :  o\n",
      "Netflix   :  org\n",
      "Inc       :  org\n",
      "announced :  o\n",
      "it        :  o\n",
      "would     :  o\n",
      "acquire   :  o\n",
      "Google    :  org\n",
      "Inc       :  org\n",
      "for       :  o\n",
      "17        :  o\n",
      "dollars   :  o\n",
      ".         :  o\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# 1. Come up with a sentence\n",
    "# 2. preprocess it for consumption by the network with `preprocess_sentence`\n",
    "# 3. use the model to make predictions\n",
    "# 4. Turn the predicted probabilities into predicted labels\n",
    "# 5. print the output nicely  (done for you)\n",
    "\n",
    "\n",
    "# Make up some of your own sentences\n",
    "sentence = 'Eric Schmidt quit after Netflix Inc announced it would acquire Google Inc for 17 dollars .'\n",
    "\n",
    "processed_sentence = preprocess_sentence(sentence)\n",
    "#predictions = model.predict...\n",
    "#predicted_labels = ...\n",
    "\n",
    "maxlen = max(map(len, sentence.split()))\n",
    "for word, label in zip(sentence.split(), predicted_labels):\n",
    "    print('{} :  {}'.format(word.ljust(maxlen), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MIT]",
   "language": "python",
   "name": "conda-env-MIT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
